{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "Activity_7_Optimizing_a_deep_learning_model.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pavankumarnani575/Technolabs-Internship/blob/main/Bitcoin%20Prediction/Activity_7_Optimizing_a_deep_learning_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHx-8QfDNcw_"
      },
      "source": [
        "# Activity 7: Optimizing a deep learning model\n",
        "In this activity we optimize our deep learning model. We aim to achieve greater performance than our model `bitcoin_lstm_v0`, which is off at about 6.8% from the real Bitcoin prices. We explore the following topics in this notebook:\n",
        "\n",
        "* Experimenting with different layers and the number of nodes\n",
        "* Grid search strategy for epoch and activation functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsVvK7EQNcxB"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2I7nvMf8NcxD",
        "outputId": "affd6968-9d3a-4217-c7f4-a8392399797b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%autosave 5\n",
        "\n",
        "#  Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sb\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.style.use('seaborn-white')\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "from keras.models import load_model, Sequential\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.layers.core import Dense, Activation, Dropout, ActivityRegularization\n",
        "from keras.callbacks import TensorBoard\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks/Technolabs/Bitcoin Prediction/scripts')\n",
        "\n",
        "from utilities_activity7 import (\n",
        "    create_groups, split_lstm_input, \n",
        "    train_model, plot_two_series, rmse, \n",
        "    mape, denormalize)\n",
        "\n",
        "np.random.seed(0)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "IPython.notebook.set_autosave_interval(5000)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Autosaving every 5 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFrpvtv7NcxM"
      },
      "source": [
        "#  Load datasets\n",
        "train = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Technolabs/Bitcoin Prediction/train_dataset.csv')\n",
        "test = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Technolabs/Bitcoin Prediction/test_dataset.csv')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tppnqlG-NcxR"
      },
      "source": [
        "#  Convert `date` column to datetime type\n",
        "test['date'] = test['date'].apply(\n",
        "    lambda x: datetime.strptime(x, '%Y-%m-%d'))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4joq4Q-NcxW"
      },
      "source": [
        "#  Group data into groups containing seven observations\n",
        "train_data = create_groups(\n",
        "    train['close_point_relative_normalization'][2:].values)\n",
        "test_data = create_groups(\n",
        "    test['close_point_relative_normalization'][:-3].values)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUSzs01dNcxc"
      },
      "source": [
        "#  Reshape the data in the format expected by the LSTM layer\n",
        "X_train, Y_train = split_lstm_input(train_data)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQCo6edpNcxh"
      },
      "source": [
        "## Reference Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "eid": "64da51",
        "id": "tMFQukwYNcxj"
      },
      "source": [
        "#  TASK:\n",
        "#  Load data for `v0` of our model.\n",
        "#  Call this `model_v0`.\n",
        "model = load_model('/content/drive/My Drive/Colab Notebooks/Technolabs/Bitcoin Prediction/bitcoin_lstm_v0.h5')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "eid": "56f346",
        "id": "LAdSzs-9Ncxo",
        "outputId": "23c529ed-8151-423b-e678-cce278cb1f10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "#  TASK:\n",
        "#  Train the reference model `model_v0`.\n",
        "#\n",
        "model_history = train_model(model=model,\n",
        "                            X=X_train, Y=Y_train,\n",
        "                            epochs=100,\n",
        "                            version=0, run_number=1)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0027\n",
            "Epoch 2/100\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
            "Instructions for updating:\n",
            "use `tf.profiler.experimental.stop` instead.\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0023\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0020\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0018\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0011\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0010\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 9.3836e-04\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.5599e-04\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 7.7978e-04\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 7.0919e-04\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 6.4378e-04\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 5.8318e-04\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 5.2708e-04\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.7520e-04\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 4.2730e-04\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 3.8314e-04\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 3.4252e-04\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.0522e-04\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.7106e-04\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 2.3985e-04\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 2.1140e-04\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.8554e-04\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 1.6211e-04\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 1.4095e-04\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 1.2189e-04\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 1.0480e-04\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 8.9544e-05\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.5981e-05\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 6.3991e-05\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.3456e-05\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.4263e-05\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.6301e-05\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.9463e-05\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 2.3646e-05\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.8749e-05\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 1.4688e-05\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.1531e-05\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 1.0097e-05\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 1.2412e-05\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 1.7883e-05\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 1.8285e-05\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 1.2160e-05\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 6.6154e-06\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 3.8794e-06\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 2.4477e-06\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 1.7140e-06\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 1.3095e-06\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.1385e-06\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.1514e-06\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 1.4106e-06\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 2.0218e-06\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.2661e-06\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 5.2686e-06\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 7.7920e-06\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 9.0623e-06\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 8.5728e-06\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 6.5701e-06\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 4.8729e-06\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.5499e-06\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 2.8421e-06\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.4448e-06\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.3782e-06\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.5124e-06\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 2.9409e-06\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.5978e-06\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.5932e-06\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 5.6325e-06\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 6.6103e-06\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 6.8661e-06\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 6.6420e-06\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.8010e-06\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 5.0351e-06\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 4.2733e-06\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.8370e-06\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 3.5432e-06\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 3.5316e-06\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 3.6441e-06\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 3.9986e-06\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.4179e-06\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.9940e-06\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 5.4246e-06\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 5.7973e-06\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.7771e-06\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.6422e-06\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 5.2157e-06\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.8727e-06\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 4.4712e-06\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.2721e-06\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 4.1087e-06\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.1531e-06\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.2299e-06\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 4.4790e-06\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.6936e-06\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.0032e-06\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 5.1545e-06\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 5.3123e-06\n",
            "CPU times: user 5.78 s, sys: 376 ms, total: 6.16 s\n",
            "Wall time: 5.68 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4SmXyoVNcxu"
      },
      "source": [
        "## Adding Layers and Nodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZnLoYDMNcxv"
      },
      "source": [
        "#  Initialize variables\n",
        "period_length = 7\n",
        "number_of_periods = 76\n",
        "batch_size = 1"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "eid": "309d7b",
        "id": "_8DxsnsmNcx0"
      },
      "source": [
        "#  Model 1: two LSTM layers\n",
        "model_v1 = Sequential()\n",
        "\n",
        "model_v1.add(LSTM(\n",
        "    units=period_length,\n",
        "    batch_input_shape=(batch_size, number_of_periods, period_length),\n",
        "    input_shape=(number_of_periods, period_length),\n",
        "    return_sequences=True, stateful=False))   # note return_sequences is now true\n",
        "\n",
        "#  TASK:\n",
        "#  Add new LSTM layer to this network here.\n",
        "#\n",
        "\n",
        "\n",
        "model_v1.add(Dense(units=period_length))\n",
        "model_v1.add(Activation(\"linear\"))\n",
        "\n",
        "model_v1.compile(loss=\"mse\", optimizer=\"rmsprop\")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a75tkI8dNcx7"
      },
      "source": [
        "%%time\n",
        "train_model(model=model_v1, X=X_train, Y=Y_train, epochs=100, version=1, run_number=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yrcpy8i3NcyA"
      },
      "source": [
        "## Epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCbELXV_NcyB"
      },
      "source": [
        "# Model 2: two LSTM layers, trained for 300 epochs\n",
        "model_v2 = Sequential()\n",
        "\n",
        "model_v2.add(LSTM(\n",
        "    units=period_length,\n",
        "    batch_input_shape=(batch_size, number_of_periods, period_length),\n",
        "    input_shape=(number_of_periods, period_length),\n",
        "    return_sequences=True, stateful=False))\n",
        "\n",
        "model_v2.add(LSTM(\n",
        "    units=period_length,\n",
        "    batch_input_shape=(batch_size, number_of_periods, period_length),\n",
        "    input_shape=(number_of_periods, period_length),\n",
        "    return_sequences=False, stateful=False))\n",
        "\n",
        "model_v2.add(Dense(units=period_length))\n",
        "model_v2.add(Activation(\"linear\"))\n",
        "\n",
        "model_v2.compile(loss=\"mse\", optimizer=\"rmsprop\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "eid": "2f8038",
        "id": "IYfHq3IyNcyG"
      },
      "source": [
        "%%time\n",
        "\n",
        "#  TASK:\n",
        "#  Change the number of epochs below\n",
        "#  to 300 and evaluate the results on TensorBoard.\n",
        "#\n",
        "train_model(model=model_v2, X=X_train, Y=Y_train, epochs=100, version=2, run_number=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAfaVYsCNcyK"
      },
      "source": [
        "## Activation Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "eid": "9e320f",
        "id": "1Rdx4cskNcyL"
      },
      "source": [
        "# Model 3: two LSTM layers, trained for 300 epochs,\n",
        "#          tanh activation function\n",
        "model_v3 = Sequential()\n",
        "\n",
        "model_v3.add(LSTM(\n",
        "    units=period_length,\n",
        "    batch_input_shape=(batch_size, number_of_periods, period_length),\n",
        "    input_shape=(number_of_periods, period_length),\n",
        "    return_sequences=True, stateful=False))\n",
        "\n",
        "model_v3.add(LSTM(\n",
        "    units=period_length,\n",
        "    batch_input_shape=(batch_size, number_of_periods, period_length),\n",
        "    input_shape=(number_of_periods, period_length),\n",
        "    return_sequences=False, stateful=False))\n",
        "\n",
        "model_v3.add(Dense(units=period_length))\n",
        "\n",
        "#  TASK:\n",
        "#  Change the activation function\n",
        "#  from \"linear\" to \"tanh\".\n",
        "#\n",
        "model_v3.add(Activation(\"linear\"))\n",
        "\n",
        "model_v3.compile(loss=\"mse\", optimizer=\"rmsprop\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uACeF3mUNcyP"
      },
      "source": [
        "%%time\n",
        "train_model(model=model_v3, X=X_train, Y=Y_train, epochs=300, version=3, run_number=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IN9PWZebNcyV"
      },
      "source": [
        "## Regularization Strategies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "eid": "87e31f",
        "id": "apNOem4PNcyW"
      },
      "source": [
        "model_v4 = Sequential()\n",
        "model_v4.add(LSTM(\n",
        "    units=period_length,\n",
        "    batch_input_shape=(batch_size, number_of_periods, period_length),\n",
        "    input_shape=(number_of_periods, period_length),\n",
        "    return_sequences=True, stateful=False))\n",
        "\n",
        "#  TASK:\n",
        "#  Implement a Dropout() here.\n",
        "#\n",
        "\n",
        "\n",
        "model_v4.add(LSTM(\n",
        "    units=period_length,\n",
        "    batch_input_shape=(batch_size, number_of_periods, period_length),\n",
        "    input_shape=(number_of_periods, period_length),\n",
        "    return_sequences=False, stateful=False))\n",
        "\n",
        "#  TASK:\n",
        "#  Implement a Dropout() here too.\n",
        "#\n",
        "\n",
        "\n",
        "model_v4.add(Dense(units=period_length))\n",
        "model_v4.add(Activation(\"tanh\"))\n",
        "\n",
        "model_v4.compile(loss=\"mse\", optimizer=\"rmsprop\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4zOxgz1Ncyb"
      },
      "source": [
        "%%time\n",
        "train_model(model=model_v4, X=X_train, Y=Y_train, epochs=600, version=4, run_number=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzOgGhscNcyf"
      },
      "source": [
        "## Evaluate Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHNcPzqqNcyg"
      },
      "source": [
        "combined_set = np.concatenate((train_data, test_data), axis=1)\n",
        "\n",
        "def evaluate_model(model, kind='series'):\n",
        "    \"\"\"Compute the MSE for all future weeks in period.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    model: Keras trained model\n",
        "    \n",
        "    kind: str, default 'series'\n",
        "        Kind of evaluation to perform. If 'series', \n",
        "        then the model will perform an evaluation \n",
        "        over the complete series.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    evaluated_weeks: list\n",
        "        List of MSE values for each evaluated\n",
        "        test week.\n",
        "    \"\"\"\n",
        "    if kind == 'series':\n",
        "        predicted_weeks = []\n",
        "        for i in range(0, test_data.shape[1]):\n",
        "            input_series = combined_set[0:,i:i+76]\n",
        "            predicted_weeks.append(model.predict(input_series))\n",
        "\n",
        "        predicted_days = []\n",
        "        for week in predicted_weeks:\n",
        "            predicted_days += list(week[0])\n",
        "\n",
        "        return predicted_days\n",
        "    else:\n",
        "        evaluated_weeks = []\n",
        "        for i in range(0, test_data.shape[1]):\n",
        "            input_series = combined_set[0:,i:i+77]\n",
        "\n",
        "            X_test = input_series[0:,:-1].reshape(1, input_series.shape[1] - 1, 7)\n",
        "            Y_test = input_series[0:,-1:][0]\n",
        "\n",
        "            result = model.evaluate(x=X_test, y=Y_test, verbose=0)\n",
        "            evaluated_weeks.append(result)\n",
        "            \n",
        "            return evaluated_weeks\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bBBDot9Ncyk"
      },
      "source": [
        "def plot_weekly_mse(series, model_name, color):\n",
        "    \"\"\"Plot weekly MSE.\"\"\"\n",
        "    ax = pd.Series(series).plot(drawstyle=\"steps-post\",\n",
        "                                figsize=(14,4),\n",
        "                                color=color,\n",
        "                                grid=True,\n",
        "                                label=model_name,\n",
        "                                alpha=0.7,\n",
        "                                title='Mean Squared Error (MSE) for Test Data (all models)'.format(\n",
        "                                       model_name))\n",
        "\n",
        "    ax.set_xticks(range(0, len(series)))\n",
        "    ax.set_xlabel(\"Predicted Week\")\n",
        "    ax.set_ylabel(\"MSE\")\n",
        "\n",
        "    return ax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGkph7TgNcyp"
      },
      "source": [
        "def plot_weekly_predictions(predicted_days, name, display_plot=True, \n",
        "                            variable='close'):\n",
        "    \"\"\"Plot weekly predictions and calculate RMSE and MAPE.\"\"\"\n",
        "    \n",
        "    # Create dataframe to store predictions and associated dates\n",
        "    last_day = datetime.strptime(train['date'].max(), '%Y-%m-%d')\n",
        "    list_of_days = []\n",
        "    for days in range(1, len(predicted_days) + 1):\n",
        "        D = (last_day + timedelta(days=days)).strftime('%Y-%m-%d')\n",
        "        list_of_days.append(D)\n",
        "    \n",
        "    predicted = pd.DataFrame({\n",
        "        'date': list_of_days,\n",
        "        'close_point_relative_normalization': predicted_days\n",
        "    })\n",
        "    \n",
        "    # Convert `date` variable to datetime\n",
        "    predicted['date'] = predicted['date'].apply(\n",
        "        lambda x: datetime.strptime(x, '%Y-%m-%d'))\n",
        "\n",
        "    # Create iso_week column in `predicted` dataframe\n",
        "    predicted['iso_week'] = predicted['date'].apply(\n",
        "        lambda x: x.strftime('%Y-%U'))\n",
        "\n",
        "    # Denormalize predictions\n",
        "    predicted_close = predicted.groupby('iso_week').apply(\n",
        "        lambda x: denormalize(test[:-3], x))\n",
        "\n",
        "    # Plot denormalized predictions and observed values\n",
        "    plot_two_series(test[:-3], predicted_close,\n",
        "                    variable=variable,\n",
        "                    title=f'{name}: Predictions per Week')\n",
        "    \n",
        "    # Calculate RMSE and MAPE\n",
        "    print(f'RMSE: {rmse(test[:-3][variable], predicted_close[variable]):.2f}')\n",
        "    print(f'MAPE: {mape(test[:-3][variable], predicted_close[variable]):.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "IFbxP6P8Ncyu"
      },
      "source": [
        "#  Evaluate each model trained in this activity in sequence\n",
        "models = [model_v0, model_v1, model_v2, model_v3, model_v4]\n",
        "for i, M in enumerate(models):\n",
        "    predicted_days = evaluate_model(M, kind='series')\n",
        "    plot_weekly_predictions(predicted_days, f'model_v{i}')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}